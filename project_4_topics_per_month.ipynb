{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/samir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/samir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import GetOldTweets3 as got\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from pymongo.errors import BulkWriteError\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import gensim, spacy, logging, warnings\n",
    "import en_core_web_sm\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('basic_cleaned_df.pkl', 'rb') as handle:\n",
    "    df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         object\n",
       "username     object\n",
       "to           object\n",
       "replies      object\n",
       "retweets     object\n",
       "favorites    object\n",
       "text         object\n",
       "mentions     object\n",
       "hashtags     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replies = df.replies.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.retweets = df.retweets.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.favorites = df.favorites.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84615 entries, 0 to 84614\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date       84615 non-null  datetime64[ns]\n",
      " 1   username   84615 non-null  object        \n",
      " 2   to         84615 non-null  object        \n",
      " 3   replies    84615 non-null  int64         \n",
      " 4   retweets   84615 non-null  int64         \n",
      " 5   favorites  84615 non-null  int64         \n",
      " 6   text       84615 non-null  object        \n",
      " 7   mentions   84615 non-null  object        \n",
      " 8   hashtags   84615 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(3), object(5)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking our URLs\n",
    "urls = lambda x: re.sub(r'http\\S+', '' ,x)\n",
    "\n",
    "#taking out capitalization and digits\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "\n",
    "#removing punctuation\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "df.text = df.text.map(urls).map(alphanumeric).map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text.drop_duplicates()\n",
    "data_list = [x for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        watch this space  wuhan is a coronavirus like sars                                                                                                                                                                                 \n",
       "1        todos mis tweets lo son                                                                                                                                                                                                            \n",
       "2        china s mystery  coronavirus  isn t currently spreading  health organization says   cbc news                                                                                                                                       \n",
       "3        china s mystery  coronavirus  isn t currently spreading  who says                                                                                                                                                                  \n",
       "4          dead    diagnosed with coronavirus related pneumonia in wuhan   health                                                                                                                                                           \n",
       "                                             ...                                                                                                                                                                                            \n",
       "84610    massachusetts officials announce   new coronavirus deaths    new cases as numbers continue to trend downward                                                                                                                       \n",
       "84611    oh my  none of these figures is sensitive or empathetic  i said they’re panicked  panic is a component in bigotry and violence  and i really meant to stick to those who unconvincingly call trump russia and coronavirus “hoaxes ”\n",
       "84612    daily coronavirus data tracker  confirmed cases up   percent  deaths remain at                                                                                                                                                     \n",
       "84613    patton oswalt has had it up to here with fox news’ coverage of the coronavirus pandemic   via  huffpostent                                                                                                                         \n",
       "84614    coronavirus  isle of wight contact tracing app trial   a mixed verdict so far                                                                                                                                                      \n",
       "Name: text, Length: 75008, dtype: object"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data = []\n",
    "for sentence in data_list:\n",
    "    data.append([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final step of getting data ready for a vectorizer\n",
    "final = []\n",
    "for sentence in data:\n",
    "    final.append(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding custom stop words for this use case\n",
    "addl_stop_words = (['coronavirus','corona virus', 'covid', 'covid-19', 'covid 19', 'corona',\n",
    "                   'virus', 'new', 'case','cases', 'deaths', 'total', 'people', 'confirmed', 'novel',\n",
    "                   'outbreak', 'pandemic', 'epidemic', 'death', 'like', 'just', 'news', 'rt', 'increasingly',\n",
    "                   'illness', 'infection', 'infected', 'diagnosed', 'reports', \"breaking\", 'reported', 'dead'\n",
    "                  ,'looks', 'know', 'big', 'type', 'make', 'unveil', 'experts', 'say', 'says', 'said', \n",
    "                    'grows', 'growing','day', 'days', \"foxnews\",'week','patient', 'hospital', 'number', \n",
    "                    'sick', 'doctor', 'next', 'health', 'first', 'even', 'press', 'youtube', 'fact', \n",
    "                    'likely', 'global', 'disease', 'thing', 'really','world', 'man', 'also', 'month', \n",
    "                    'job', 'many', 'time', 'way', 'get', 'think', 'need', 'home', 'go', 'may', 'going', 'would',\n",
    "                    'live', 'see', 'update', 'far', 'last', 'year', 'back', 'much', 'medical', 'one', 'via',\n",
    "                    'could', 'maybe', 'details', 'today', 'three', 'ninth', 'epoch', 'epoch times', 'download', 'app'\n",
    "                   ,'pron', 'daily', 'updates', 'coverage', 'fox', 'virtual', 'hall', 'programming', 'alert',\n",
    "                   'coronavirusoutbreak', 'confirm','due','die', 'gon', 'na', 'gonna', 'wan', 'wanna', 'come', 'take'\n",
    "                   , 'kill'])\n",
    "\n",
    "\n",
    "custom_stop_words = stopwords.words('english') + addl_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function taken from lecture slides to help display the topics and the top words per topic\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics surrounding quarantine and gov't shutdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we get into months, I wanted to look at the discussion around the gov't shutdown in particular to see what the topics are there**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarantine = (df['text'].str.contains(\"quarantine\"))\n",
    "shutdown = (df['text'].str.contains(\"shutdown\"))\n",
    "lockdown = (df['text'].str.contains(\"lockdown\"))\n",
    "shut_down = (df['text'].str.contains(\"shut down\"))\n",
    "lock_down = (df['text'].str.contains(\"lock down\"))\n",
    "shelter1 = (df['text'].str.contains(\"shelter in-place\"))\n",
    "shelter2 = (df['text'].str.contains(\"shelter inplace\"))\n",
    "shelter3 = (df['text'].str.contains(\"shelter in place\"))\n",
    "social_distancing = (df['text'].str.contains(\"social distancing\"))\n",
    "\n",
    "mask = quarantine | shutdown | lockdown | shut_down | lock_down | shelter1 | shelter2 | shelter3 | social_distancing\n",
    "\n",
    "quarantine_df = df[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = quarantine_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding custom stop words for this use case\n",
    "quarantine_words = ['lockdown', 'shutdown', 'shut down', 'lock down', 'quarantine', 'shelter', 'in-place', 'social distancing', 'social', 'distancing']\n",
    "\n",
    "quarantine_stop_words = custom_stop_words + quarantine_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'lock', 'place', 'shut', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "shut transportation, transportation wuhan, shut transportation wuhan, transportation, authorities limit, travel million, travel million residents, limit travel, limit travel million, authorities limit travel\n",
      "\n",
      "Topic  1\n",
      "april, guidelines, extends, trump, guidelines april, extends guidelines, extends guidelines april, trump extends, trump extends guidelines, peak\n",
      "\n",
      "Topic  2\n",
      "authoritarian orders, persist authoritarian, persist authoritarian orders, governors persist, governors persist authoritarian, barr governors, barr governors persist, ag barr governors, ag, ag barr\n",
      "\n",
      "Topic  3\n",
      "disney, furlough, co, walt disney, disney co furlough, disney co, co furlough workers, co furlough, walt, furlough workers\n",
      "\n",
      "Topic  4\n",
      "mental, practices, asked stay, practices help, collective mental uncertain, psychologically practices, psychologically, easy psychologically practices, easy psychologically, asked stay fight\n",
      "\n",
      "Topic  5\n",
      "italian towns, towns, italian, italian towns fears, towns fears, fears, towns fears yahoo, fears yahoo, yahoo, italy\n",
      "\n",
      "Topic  6\n",
      "quarantined, tested, positive, tested positive, according, self, china quarantined, self quarantined, police, least\n",
      "\n",
      "Topic  7\n",
      "shut, threat, shut threat, trump shut, shuns, shut threat china, threat china, threat china shuns, china shuns, trump shut threat\n",
      "\n",
      "Topic  8\n",
      "lockdowns, us, amp, work, quarantinelife, shut, end, stay, states, self\n",
      "\n",
      "Topic  9\n",
      "socialdistancing, coronalockdown, stayathomeandstaysafe, socialdistancing coronalockdown, music, listen, socialdistancing coronalockdown stayathomeandstaysafe, coronalockdown stayathomeandstaysafe, great, link\n",
      "\n",
      "Topic  10\n",
      "king, tiger, tiger king, joe, exotic, joe exotic, joe exotic prison, exotic prison, prison, star\n",
      "\n",
      "Topic  11\n",
      "china, wuhan, city, chinese, wuhan china, million, toll, spread, chinese city, imposes\n",
      "\n",
      "Topic  12\n",
      "order, los angeles, angeles, los, extend, extend order, angeles extend order, angeles extend, los angeles extend, extend order august\n",
      "\n",
      "Topic  13\n",
      "india, migrant, migrant workers, workers, india desperate migrant, desperate migrant workers, desperate migrant, india desperate, workers trapped, migrant workers trapped\n",
      "\n",
      "Topic  14\n",
      "cruise, ship, cruise ship, princess, diamond, diamond princess, quarantined cruise, quarantined cruise ship, princess cruise, diamond princess cruise\n"
     ]
    }
   ],
   "source": [
    "qte_nmf_tfidf = TfidfVectorizer(stop_words=quarantine_stop_words,ngram_range=(1,3), min_df=5, binary=True)\n",
    "qte_tfidf_nmf_doc_word = qte_nmf_tfidf.fit_transform(X_new)\n",
    "qte_nmf_model = NMF(15)\n",
    "qte_tfidf_nmf_doc_topic = qte_nmf_model.fit_transform(qte_tfidf_nmf_doc_word)\n",
    "display_topics(qte_nmf_model, qte_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The shutdown topics are interesting! Tiger King made an appearance. But nothing too juicy here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings from Today:**\n",
    "1. There are so many topics that even 40 topics doesn't seem like enough\n",
    "1. I found the news headline topics\n",
    "1. I haven't been able to find the \"regular people\" topics. Either that or those topics mirror the overal topics I found\n",
    "1. I found topics around quarantining and found interesting results\n",
    "1. Even with 40 topics, I'm still only explaining 5% of variance\n",
    "1. I think I'm gotten to a point where I have stable topics\n",
    "\n",
    "\n",
    "**Still need to do:**\n",
    "1. Find a business use case or some interesting angle\n",
    "1. see how topics changed over time\n",
    "1. sentiment analysis\n",
    "1. clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeing how topics changed over time with TF-IDF and NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_df = df[df.date.apply(lambda x: x.month==12)]\n",
    "jan_df = df[df.date.apply(lambda x: x.month==1)]\n",
    "feb_df = df[df.date.apply(lambda x: x.month==2)]\n",
    "mar_df = df[df.date.apply(lambda x: x.month==3)]\n",
    "apr_df = df[df.date.apply(lambda x: x.month==4)]\n",
    "may_df = df[df.date.apply(lambda x: x.month==5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dec Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "qatar, cov qatar, mers cov qatar, syndrome mers cov, syndrome mers, respiratory syndrome mers, mers cov, cov, middle east respiratory, middle east\n",
      "\n",
      "Topic  1\n",
      "dromedary, dromedary camels, camels, imported, local, camels prospective, dromedary camels prospective, local arabian dromedary, genomic study, imported african local\n",
      "\n",
      "Topic  2\n",
      "kingdom saudi arabia, kingdom, kingdom saudi, saudi, saudi arabia, arabia, mers cov kingdom, cov kingdom, cov kingdom saudi, syndrome mers\n",
      "\n",
      "Topic  3\n",
      "likes retweets, likes, twitter mentions, mentions, reach, retweets, twitter, mention reach, reach likes, mentions mention reach\n",
      "\n",
      "Topic  4\n",
      "exacto, amen, real, bro, agenparl, socialnetwork, siu lam, siu, agenparlenglish socialnetwork, agenparlenglish\n",
      "\n",
      "Topic  5\n",
      "feline, feline infectious, feline infectious peritonitis, infectious peritonitis, peritonitis, infectious, cats, fip, caused, infectious peritonitis fip\n",
      "\n",
      "Topic  6\n",
      "biggest fans, thank, biggest, fans, pedromcasals, biggest fans pedromcasals, fans pedromcasals, pedromcasals rototten urielsuriel, rototten urielsuriel thank, pedromcasals rototten\n",
      "\n",
      "Topic  7\n",
      "geopositioned, database, occurrences, geopositioned middle, geopositioned middle east, database geopositioned, database geopositioned middle, syndrome occurrences, scientific data, respiratory syndrome occurrences\n",
      "\n",
      "Topic  8\n",
      "national ihr focal, focal point, ihr, ihr focal, ihr focal point, focal, national ihr, national, point, additional middle east\n",
      "\n",
      "Topic  9\n",
      "humanity, molecular mechanism, molecular, enhancement entry, enhancement, antibody dependent enhancement, antibody dependent, antibody, dependent enhancement entry, molecular mechanism antibody\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "data = dec_df.text\n",
    "dec_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "dec_tfidf_nmf_doc_word = dec_nmf_tfidf.fit_transform(data)\n",
    "dec_nmf_model = NMF(10)\n",
    "dec_tfidf_nmf_doc_topic = dec_nmf_model.fit_transform(dec_tfidf_nmf_doc_word)\n",
    "display_topics(dec_nmf_model, dec_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jan Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "china, cause china, china wuhan, china pneumonia, wuhan china, china worse, worse, mystery china, strain, cause\n",
      "\n",
      "Topic  1\n",
      "mystery caused, wuhan pneumonia mystery, pneumonia mystery caused, pneumonia mystery, mystery, wuhan pneumonia, caused, pneumonia, wuhan, bbc\n",
      "\n",
      "Topic  2\n",
      "shut transportation, shut transportation wuhan, authorities limit, limit travel, residents shut transportation, residents shut, travel million residents, travel million, authorities limit travel, transportation wuhan amid\n",
      "\n",
      "Topic  3\n",
      "chinese city, city, city wuhan, chinese city wuhan, central, chinese, central chinese city, central chinese, authorities central chinese, authorities central\n",
      "\n",
      "Topic  4\n",
      "chinese report, report, illnesses, chinese report illnesses, report illnesses, chinese, xpress, chinese report wuhan, report wuhan, china chinese report\n",
      "\n",
      "Topic  5\n",
      "hacked, hacked phone hits, hacked phone, bezos hacked phone, hits us, phone hits, phone hits us, bezos hacked, jeff bezos hacked, bezos\n",
      "\n",
      "Topic  6\n",
      "wuhan, pneumonia, pneumonia wuhan, wuhan china, cause, pneumonia wuhan china, sars, wuhan pneumonia, related, identifies\n",
      "\n",
      "Topic  7\n",
      "respiratory syndrome, syndrome, respiratory, east respiratory syndrome, middle east respiratory, east respiratory, middle east, east, middle, mers\n",
      "\n",
      "Topic  8\n",
      "public transport, shut public transport, shut public, transport, shut, public, wuhan shut public, wuhan shut, bbc, wuhan\n",
      "\n",
      "Topic  9\n",
      "china pneumonia, pneumonia, linked, pneumonia linked, mystery, mystery pneumonia, pneumonia china, china mystery, china pneumonia linked, mystery pneumonia linked\n"
     ]
    }
   ],
   "source": [
    "data = jan_df.text.drop_duplicates()\n",
    "jan_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "jan_tfidf_nmf_doc_word = jan_nmf_tfidf.fit_transform(data)\n",
    "jan_nmf_model = NMF(10)\n",
    "jan_tfidf_nmf_doc_topic = jan_nmf_model.fit_transform(jan_tfidf_nmf_doc_word)\n",
    "display_topics(jan_nmf_model, jan_nmf_tfidf.get_feature_names(), 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feb Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "sars flu, gt sars, gt sars flu, sars, gt, flu, flu china, sars flu china, china trump, trump\n",
      "\n",
      "Topic  1\n",
      "italy, northern italy, italy least, northern, least, ansa, italy italy, italy ansa, monselice, old\n",
      "\n",
      "Topic  2\n",
      "hubei, china hubei, hubei province, province, china hubei province, china, feb, hubei province epicentre, province epicentre, epicentre\n",
      "\n",
      "Topic  3\n",
      "mobile, mobile congress, congress, fears, canceled, cancelled, concerns, mobile congress cancelled, congress cancelled, congress canceled\n",
      "\n",
      "Topic  4\n",
      "us, cdc, americans, spread, flown, americans flown, advice, cdc advice, americans flown cdc, flown cdc\n",
      "\n",
      "Topic  5\n",
      "china, outside, outside china, philippines, philippines outside china, philippines outside, korea, south, south korea, related outside\n",
      "\n",
      "Topic  6\n",
      "test, kits, states, test kits, kits sent, flawed, kits sent states, sent states, sent, test kits sent\n",
      "\n",
      "Topic  7\n",
      "got, bitch got, bitch, shit got, shit, nigga, wtf, bro, got dat, nigga got\n",
      "\n",
      "Topic  8\n",
      "bay, bay area, area, san francisco, francisco, san, francisco bay area, francisco bay, san francisco bay, california\n",
      "\n",
      "Topic  9\n",
      "wuhan, china, wuhan china, recovered, toll, end, predict, predict end, chinese, early predict\n"
     ]
    }
   ],
   "source": [
    "data = feb_df.text.drop_duplicates()\n",
    "feb_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "feb_tfidf_nmf_doc_word = feb_nmf_tfidf.fit_transform(data)\n",
    "feb_nmf_model = NMF(10)\n",
    "feb_tfidf_nmf_doc_topic = feb_nmf_model.fit_transform(feb_tfidf_nmf_doc_word)\n",
    "display_topics(feb_nmf_model, feb_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## March Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "weinstein, harvey weinstein, harvey, weinstein tests positive, weinstein tests, harvey weinstein tests, tests positive, tests, prison, positive\n",
      "\n",
      "Topic  1\n",
      "distancing, social distancing, social, guidelines, april, distancing guidelines, social distancing guidelines, extends, guidelines april, distancing guidelines april\n",
      "\n",
      "Topic  2\n",
      "fails, fails move, fails move forward, move forward, move, bill, forward, senate fails, senate fails move, move forward phase\n",
      "\n",
      "Topic  3\n",
      "trump, us, president, response, china, got, help, stop, crisis, americans\n",
      "\n",
      "Topic  4\n",
      "positive, tests, tests positive, tested, son, atiku, tested positive, son tests positive, son tests, atiku son\n",
      "\n",
      "Topic  5\n",
      "diffie, joe diffie, joe, country, dies, complications, star joe, star joe diffie, diffie dies, star\n",
      "\n",
      "Topic  6\n",
      "john prine, prine, john, critical condition, condition, critical, john prine critical, prine critical condition, prine critical, condition symptoms\n",
      "\n",
      "Topic  7\n",
      "president, president trump, teacher offers, offers pay, pay someone cough, someone cough, offers pay someone, pay someone, teacher offers pay, offers\n",
      "\n",
      "Topic  8\n",
      "fuck, shit, bro, fuck bro, got, fucking, nigga, bitch, fuck nigga, bullshit\n",
      "\n",
      "Topic  9\n",
      "senate, bill, short votes needed, short votes, votes needed, needed advance, votes needed advance, senate falls short, needed advance bill, falls short\n"
     ]
    }
   ],
   "source": [
    "data = mar_df.text.drop_duplicates()\n",
    "mar_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "mar_tfidf_nmf_doc_word = mar_nmf_tfidf.fit_transform(data)\n",
    "mar_nmf_model = NMF(10)\n",
    "mar_tfidf_nmf_doc_topic = mar_nmf_model.fit_transform(mar_tfidf_nmf_doc_word)\n",
    "display_topics(mar_nmf_model, mar_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## April Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "wave, second wave, warns, cdc director, second, director, cdc director warns, director warns, cdc, warns second\n",
      "\n",
      "Topic  1\n",
      "us, china, help, vaccine, test, spread, lockdown, crisis, state, states\n",
      "\n",
      "Topic  2\n",
      "white, task, white house, house, task force, force, briefing, force briefing, task force briefing, white house task\n",
      "\n",
      "Topic  3\n",
      "senate, passes, billion, senate passes, relief, bill, small, passes billion, senate passes billion, package\n",
      "\n",
      "Topic  4\n",
      "contract, dem, consultant trump, backtracks, awarded dem consultant, contract awarded dem, trump asap, consultant trump asap, backtracks contract, backtracks contract awarded\n",
      "\n",
      "Topic  5\n",
      "navy, carrier, aircraft carrier, aircraft, captain, alarm, raised alarm, raised, captain raised, captain raised alarm\n",
      "\n",
      "Topic  6\n",
      "trump, immigration, immigration suspension, suspension, trump immigration, trump immigration suspension, suspend immigration, suspend, trump suspend immigration, trump suspend\n",
      "\n",
      "Topic  7\n",
      "possibly worse, possibly worse winter, worse winter, cdc chief, second possibly worse, second possibly, chief, possibly, worse, chief second\n",
      "\n",
      "Topic  8\n",
      "mutated, strains, finds, study, mutated least, mutated least different, least different, least different strains, different strains, study finds\n",
      "\n",
      "Topic  9\n",
      "million, worldwide, million worldwide, top, top million, pass million, pass, surpass million, surpass, worldwide probably\n"
     ]
    }
   ],
   "source": [
    "data = apr_df.text.drop_duplicates()\n",
    "apr_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "apr_tfidf_nmf_doc_word = apr_nmf_tfidf.fit_transform(data)\n",
    "apr_nmf_model = NMF(10)\n",
    "apr_tfidf_nmf_doc_topic = apr_nmf_model.fit_transform(apr_tfidf_nmf_doc_word)\n",
    "display_topics(apr_nmf_model, apr_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "deregulations amid, deregulations, order aiming, announces executive order, announces executive, executive order aiming, order aiming hundreds, aiming hundreds, aiming, executive order\n",
      "\n",
      "Topic  1\n",
      "cost largest, bill estimated, bill estimated cost, estimated cost, estimated cost largest, cost largest stimulus, estimated, largest stimulus package, largest stimulus, package yet\n",
      "\n",
      "Topic  2\n",
      "democrats, relief, trillion, house democrats, bill, house, trillion relief, democrats trillion, house democrats trillion, democrats trillion relief\n",
      "\n",
      "Topic  3\n",
      "trump, fauci, response, dr, china, president, uk, testing, died, care\n",
      "\n",
      "Topic  4\n",
      "egregious misreading data, misreading data, data survey, misreading data survey, misreading, egregious misreading, egregious, cnn accused, cnn accused egregious, accused egregious\n",
      "\n",
      "Topic  5\n",
      "bringing coronavirusoutbreak, coronavirusoutbreak, bringing, usa bringing, usa bringing coronavirusoutbreak, texas bringing coronavirusoutbreak, texas bringing, ohio bringing, ohio bringing coronavirusoutbreak, ohio\n",
      "\n",
      "Topic  6\n",
      "illinois, stake companies involved, companies involved testing, companies involved, owns stake companies, governor family firm, illinois governor family, family firm, firm owns stake, stake companies\n",
      "\n",
      "Topic  7\n",
      "vaccine, treatment ready, states, vaccine treatment, states full scale, states full, scale reopening, vaccine treatment ready, scale reopening vaccine, full scale reopening\n",
      "\n",
      "Topic  8\n",
      "us, august, model, us august, model projects, projects, projects us august, projects us, model projects us, key model\n",
      "\n",
      "Topic  9\n",
      "white house, white, house, rates spiking, white house report, house report, spiking, spiking heartland, rates spiking heartland, heartland\n"
     ]
    }
   ],
   "source": [
    "data = may_df.text.drop_duplicates()\n",
    "may_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "may_tfidf_nmf_doc_word = may_nmf_tfidf.fit_transform(data)\n",
    "may_nmf_model = NMF(10)\n",
    "may_tfidf_nmf_doc_topic = may_nmf_model.fit_transform(may_tfidf_nmf_doc_word)\n",
    "display_topics(may_nmf_model, may_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "Not too much additional data here. It does show when certain topics are the most popular though. The next stop is to create a plot showing how topics changed over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
