{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/samir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/samir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import GetOldTweets3 as got\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from pymongo.errors import BulkWriteError\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import gensim, spacy, logging, warnings\n",
    "import en_core_web_sm\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('basic_cleaned_df.pkl', 'rb') as handle:\n",
    "    df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         object\n",
       "username     object\n",
       "to           object\n",
       "replies      object\n",
       "retweets     object\n",
       "favorites    object\n",
       "text         object\n",
       "mentions     object\n",
       "hashtags     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replies = df.replies.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.retweets = df.retweets.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.favorites = df.favorites.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84615 entries, 0 to 84614\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date       84615 non-null  datetime64[ns]\n",
      " 1   username   84615 non-null  object        \n",
      " 2   to         84615 non-null  object        \n",
      " 3   replies    84615 non-null  int64         \n",
      " 4   retweets   84615 non-null  int64         \n",
      " 5   favorites  84615 non-null  int64         \n",
      " 6   text       84615 non-null  object        \n",
      " 7   mentions   84615 non-null  object        \n",
      " 8   hashtags   84615 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(3), object(5)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking our URLs\n",
    "urls = lambda x: re.sub(r'http\\S+', '' ,x)\n",
    "\n",
    "#taking out capitalization and digits\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "\n",
    "#removing punctuation\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "df.text = df.text.map(urls).map(alphanumeric).map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text.drop_duplicates()\n",
    "data_list = [x for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data = []\n",
    "for sentence in data_list:\n",
    "    data.append([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final step of getting data ready for a vectorizer\n",
    "final = []\n",
    "for sentence in data:\n",
    "    final.append(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding custom stop words for this use case\n",
    "addl_stop_words = (['coronavirus','corona virus', 'covid', 'covid-19', 'covid 19', 'corona',\n",
    "                   'virus', 'new', 'case','cases', 'deaths', 'total', 'people', 'confirmed', 'novel',\n",
    "                   'outbreak', 'pandemic', 'epidemic', 'death', 'like', 'just', 'news', 'rt', 'increasingly',\n",
    "                   'illness', 'infection', 'infected', 'diagnosed', 'reports', \"breaking\", 'reported', 'dead'\n",
    "                  ,'looks', 'know', 'big', 'type', 'make', 'unveil', 'experts', 'say', 'says', 'said', \n",
    "                    'grows', 'growing','day', 'days', \"foxnews\",'week','patient', 'hospital', 'number', \n",
    "                    'sick', 'doctor', 'next', 'health', 'first', 'even', 'press', 'youtube', 'fact', \n",
    "                    'likely', 'global', 'disease', 'thing', 'really','world', 'man', 'also', 'month', \n",
    "                    'job', 'many', 'time', 'way', 'get', 'think', 'need', 'home', 'go', 'may', 'going', 'would',\n",
    "                    'live', 'see', 'update', 'far', 'last', 'year', 'back', 'much', 'medical', 'one', 'via',\n",
    "                    'could', 'maybe', 'details', 'today', 'three', 'ninth', 'epoch', 'epoch times', 'download', 'app'\n",
    "                   ,'pron', 'daily', 'updates', 'coverage', 'fox', 'virtual', 'hall', 'programming', 'alert',\n",
    "                   'coronavirusoutbreak', 'confirm','due','die', 'gon', 'na', 'gonna', 'wan', 'wanna', 'come', 'take'\n",
    "                   , 'kill'])\n",
    "\n",
    "\n",
    "custom_stop_words = stopwords.words('english') + addl_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function taken from lecture slides to help display the topics and the top words per topic\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics surrounding quarantine and gov't shutdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we get into months, I wanted to look at the discussion around the gov't shutdown in particular to see what the topics are there**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarantine = (df['text'].str.contains(\"quarantine\"))\n",
    "shutdown = (df['text'].str.contains(\"shutdown\"))\n",
    "lockdown = (df['text'].str.contains(\"lockdown\"))\n",
    "shut_down = (df['text'].str.contains(\"shut down\"))\n",
    "lock_down = (df['text'].str.contains(\"lock down\"))\n",
    "shelter1 = (df['text'].str.contains(\"shelter in-place\"))\n",
    "shelter2 = (df['text'].str.contains(\"shelter inplace\"))\n",
    "shelter3 = (df['text'].str.contains(\"shelter in place\"))\n",
    "social_distancing = (df['text'].str.contains(\"social distancing\"))\n",
    "\n",
    "mask = quarantine | shutdown | lockdown | shut_down | lock_down | shelter1 | shelter2 | shelter3 | social_distancing\n",
    "\n",
    "quarantine_df = df[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = quarantine_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding custom stop words for this use case\n",
    "quarantine_words = ['lockdown', 'shutdown', 'shut down', 'lock down', 'quarantine', 'shelter', 'in-place', 'social distancing', 'social', 'distancing']\n",
    "\n",
    "quarantine_stop_words = custom_stop_words + quarantine_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'lock', 'place', 'shut', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "shut transportation, transportation wuhan, shut transportation wuhan, transportation, authorities limit, travel million, travel million residents, limit travel, limit travel million, authorities limit travel\n",
      "\n",
      "Topic  1\n",
      "april, guidelines, extends, trump, guidelines april, extends guidelines, extends guidelines april, trump extends, trump extends guidelines, peak\n",
      "\n",
      "Topic  2\n",
      "authoritarian orders, persist authoritarian, persist authoritarian orders, governors persist, governors persist authoritarian, barr governors, barr governors persist, ag barr governors, ag, ag barr\n",
      "\n",
      "Topic  3\n",
      "disney, furlough, co, walt disney, disney co furlough, disney co, co furlough workers, co furlough, walt, furlough workers\n",
      "\n",
      "Topic  4\n",
      "mental, practices, asked stay, practices help, collective mental uncertain, psychologically practices, psychologically, easy psychologically practices, easy psychologically, asked stay fight\n",
      "\n",
      "Topic  5\n",
      "italian towns, towns, italian, italian towns fears, towns fears, fears, towns fears yahoo, fears yahoo, yahoo, italy\n",
      "\n",
      "Topic  6\n",
      "quarantined, tested, positive, tested positive, according, self, china quarantined, self quarantined, police, least\n",
      "\n",
      "Topic  7\n",
      "shut, threat, shut threat, trump shut, shuns, shut threat china, threat china, threat china shuns, china shuns, trump shut threat\n",
      "\n",
      "Topic  8\n",
      "lockdowns, us, amp, work, quarantinelife, shut, end, stay, states, self\n",
      "\n",
      "Topic  9\n",
      "socialdistancing, coronalockdown, stayathomeandstaysafe, socialdistancing coronalockdown, music, listen, socialdistancing coronalockdown stayathomeandstaysafe, coronalockdown stayathomeandstaysafe, great, link\n",
      "\n",
      "Topic  10\n",
      "king, tiger, tiger king, joe, exotic, joe exotic, joe exotic prison, exotic prison, prison, star\n",
      "\n",
      "Topic  11\n",
      "china, wuhan, city, chinese, wuhan china, million, toll, spread, chinese city, imposes\n",
      "\n",
      "Topic  12\n",
      "order, los angeles, angeles, los, extend, extend order, angeles extend order, angeles extend, los angeles extend, extend order august\n",
      "\n",
      "Topic  13\n",
      "india, migrant, migrant workers, workers, india desperate migrant, desperate migrant workers, desperate migrant, india desperate, workers trapped, migrant workers trapped\n",
      "\n",
      "Topic  14\n",
      "cruise, ship, cruise ship, princess, diamond, diamond princess, quarantined cruise, quarantined cruise ship, princess cruise, diamond princess cruise\n"
     ]
    }
   ],
   "source": [
    "qte_nmf_tfidf = TfidfVectorizer(stop_words=quarantine_stop_words,ngram_range=(1,3), min_df=5, binary=True)\n",
    "qte_tfidf_nmf_doc_word = qte_nmf_tfidf.fit_transform(X_new)\n",
    "qte_nmf_model = NMF(15)\n",
    "qte_tfidf_nmf_doc_topic = qte_nmf_model.fit_transform(qte_tfidf_nmf_doc_word)\n",
    "display_topics(qte_nmf_model, qte_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The shutdown topics are interesting and Tiger King made an appearance. But nothing too juicy here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeing how topics changed over time with TF-IDF and NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_df = df[df.date.apply(lambda x: x.month==12)]\n",
    "jan_df = df[df.date.apply(lambda x: x.month==1)]\n",
    "feb_df = df[df.date.apply(lambda x: x.month==2)]\n",
    "mar_df = df[df.date.apply(lambda x: x.month==3)]\n",
    "apr_df = df[df.date.apply(lambda x: x.month==4)]\n",
    "may_df = df[df.date.apply(lambda x: x.month==5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dec Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "qatar, cov qatar, mers cov qatar, syndrome mers cov, syndrome mers, respiratory syndrome mers, mers cov, cov, middle east respiratory, middle east\n",
      "\n",
      "Topic  1\n",
      "dromedary, dromedary camels, camels, imported, local, camels prospective, dromedary camels prospective, local arabian dromedary, genomic study, imported african local\n",
      "\n",
      "Topic  2\n",
      "kingdom saudi arabia, kingdom, kingdom saudi, saudi, saudi arabia, arabia, mers cov kingdom, cov kingdom, cov kingdom saudi, syndrome mers\n",
      "\n",
      "Topic  3\n",
      "likes, retweets, mentions, reach, likes retweets, twitter mentions, twitter, reach likes retweets, twitter mentions mention, mention reach\n",
      "\n",
      "Topic  4\n",
      "exacto, amen, real, molecular mechanism, molecular mechanism antibody, mechanism antibody dependent, mechanism antibody, mechanism, antibody, molecular\n",
      "\n",
      "Topic  5\n",
      "feline, feline infectious, feline infectious peritonitis, infectious peritonitis, peritonitis, infectious, cats, fip, caused, infectious peritonitis fip\n",
      "\n",
      "Topic  6\n",
      "biggest fans, biggest, thank, fans, pedromcasals, biggest fans pedromcasals, fans pedromcasals, urielsuriel, urielsuriel thank, rototten\n",
      "\n",
      "Topic  7\n",
      "database, geopositioned, occurrences, geopositioned middle east, geopositioned middle, database geopositioned middle, database geopositioned, scientific data, respiratory syndrome occurrences, syndrome occurrences\n",
      "\n",
      "Topic  8\n",
      "national ihr focal, focal point, ihr, ihr focal, ihr focal point, focal, national ihr, national, point, additional middle east\n",
      "\n",
      "Topic  9\n",
      "humanity, real, bro, real bro, hi bro, hi, increase infectivity, syndrome host, syndrome host cells, cells increase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "data = dec_df.text\n",
    "dec_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "dec_tfidf_nmf_doc_word = dec_nmf_tfidf.fit_transform(data)\n",
    "dec_nmf_model = NMF(10)\n",
    "dec_tfidf_nmf_doc_topic = dec_nmf_model.fit_transform(dec_tfidf_nmf_doc_word)\n",
    "display_topics(dec_nmf_model, dec_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jan Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "china, cause china, china wuhan, china pneumonia, wuhan china, china worse, worse, mystery china, strain, cause\n",
      "\n",
      "Topic  1\n",
      "mystery caused, wuhan pneumonia mystery, pneumonia mystery caused, pneumonia mystery, mystery, wuhan pneumonia, caused, pneumonia, wuhan, bbc\n",
      "\n",
      "Topic  2\n",
      "shut transportation, shut transportation wuhan, authorities limit, limit travel, residents shut transportation, residents shut, travel million residents, travel million, authorities limit travel, transportation wuhan amid\n",
      "\n",
      "Topic  3\n",
      "chinese city, city, city wuhan, chinese city wuhan, central, chinese, central chinese city, central chinese, authorities central chinese, authorities central\n",
      "\n",
      "Topic  4\n",
      "chinese report, report, illnesses, chinese report illnesses, report illnesses, chinese, xpress, chinese report wuhan, report wuhan, china chinese report\n",
      "\n",
      "Topic  5\n",
      "hacked, hacked phone hits, hacked phone, bezos hacked phone, hits us, phone hits, phone hits us, bezos hacked, jeff bezos hacked, bezos\n",
      "\n",
      "Topic  6\n",
      "wuhan, pneumonia, pneumonia wuhan, wuhan china, cause, pneumonia wuhan china, sars, wuhan pneumonia, related, identifies\n",
      "\n",
      "Topic  7\n",
      "respiratory syndrome, syndrome, respiratory, east respiratory syndrome, middle east respiratory, east respiratory, middle east, east, middle, mers\n",
      "\n",
      "Topic  8\n",
      "public transport, shut public transport, shut public, transport, shut, public, wuhan shut public, wuhan shut, bbc, wuhan\n",
      "\n",
      "Topic  9\n",
      "china pneumonia, pneumonia, linked, pneumonia linked, mystery, mystery pneumonia, pneumonia china, china mystery, china pneumonia linked, mystery pneumonia linked\n"
     ]
    }
   ],
   "source": [
    "data = jan_df.text.drop_duplicates()\n",
    "jan_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "jan_tfidf_nmf_doc_word = jan_nmf_tfidf.fit_transform(data)\n",
    "jan_nmf_model = NMF(10)\n",
    "jan_tfidf_nmf_doc_topic = jan_nmf_model.fit_transform(jan_tfidf_nmf_doc_word)\n",
    "display_topics(jan_nmf_model, jan_nmf_tfidf.get_feature_names(), 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feb Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "sars flu, gt sars, gt sars flu, sars, gt, flu, flu china, sars flu china, china trump, trump\n",
      "\n",
      "Topic  1\n",
      "italy, northern italy, italy least, northern, least, ansa, italy italy, italy ansa, monselice, old\n",
      "\n",
      "Topic  2\n",
      "hubei, china hubei, hubei province, province, china hubei province, china, feb, hubei province epicentre, province epicentre, epicentre\n",
      "\n",
      "Topic  3\n",
      "mobile, mobile congress, congress, fears, canceled, cancelled, concerns, congress cancelled, mobile congress cancelled, mobile congress canceled\n",
      "\n",
      "Topic  4\n",
      "us, cdc, americans, spread, flown, americans flown, advice, cdc advice, flown cdc, americans flown cdc\n",
      "\n",
      "Topic  5\n",
      "china, outside, outside china, philippines, philippines outside, philippines outside china, korea, south, south korea, related outside china\n",
      "\n",
      "Topic  6\n",
      "test, kits, states, test kits, kits sent, flawed, kits sent states, sent states, sent, test kits sent\n",
      "\n",
      "Topic  7\n",
      "got, bitch got, bitch, shit got, shit, nigga, wtf, bro, got dat, nigga got\n",
      "\n",
      "Topic  8\n",
      "bay, bay area, area, san francisco, francisco, san, san francisco bay, francisco bay, francisco bay area, california\n",
      "\n",
      "Topic  9\n",
      "wuhan, china, wuhan china, recovered, end, toll, predict, predict end, early predict, early predict end\n"
     ]
    }
   ],
   "source": [
    "data = feb_df.text.drop_duplicates()\n",
    "feb_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "feb_tfidf_nmf_doc_word = feb_nmf_tfidf.fit_transform(data)\n",
    "feb_nmf_model = NMF(10)\n",
    "feb_tfidf_nmf_doc_topic = feb_nmf_model.fit_transform(feb_tfidf_nmf_doc_word)\n",
    "display_topics(feb_nmf_model, feb_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## March Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "weinstein, harvey weinstein, harvey, weinstein tests positive, weinstein tests, harvey weinstein tests, tests positive, tests, prison, positive\n",
      "\n",
      "Topic  1\n",
      "distancing, social distancing, social, guidelines, april, distancing guidelines, social distancing guidelines, extends, guidelines april, distancing guidelines april\n",
      "\n",
      "Topic  2\n",
      "fails, fails move, fails move forward, move forward, move, bill, forward, senate fails, senate fails move, move forward phase\n",
      "\n",
      "Topic  3\n",
      "trump, us, president, response, china, got, help, stop, crisis, americans\n",
      "\n",
      "Topic  4\n",
      "positive, tests, tests positive, tested, son, atiku, tested positive, son tests, son tests positive, atiku son\n",
      "\n",
      "Topic  5\n",
      "diffie, joe diffie, joe, country, dies, complications, star joe diffie, star joe, diffie dies, star\n",
      "\n",
      "Topic  6\n",
      "john prine, prine, john, critical condition, condition, critical, john prine critical, prine critical, prine critical condition, critical condition symptoms\n",
      "\n",
      "Topic  7\n",
      "president, president trump, someone cough, pay someone cough, pay someone, offers pay someone, offers pay, teacher offers, teacher offers pay, offers\n",
      "\n",
      "Topic  8\n",
      "fuck, shit, bro, fuck bro, got, nigga, fucking, bitch, fuck nigga, bullshit\n",
      "\n",
      "Topic  9\n",
      "senate, bill, short votes, short votes needed, votes needed, needed advance, votes needed advance, advance bill, senate falls, senate falls short\n"
     ]
    }
   ],
   "source": [
    "data = mar_df.text.drop_duplicates()\n",
    "mar_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "mar_tfidf_nmf_doc_word = mar_nmf_tfidf.fit_transform(data)\n",
    "mar_nmf_model = NMF(10)\n",
    "mar_tfidf_nmf_doc_topic = mar_nmf_model.fit_transform(mar_tfidf_nmf_doc_word)\n",
    "display_topics(mar_nmf_model, mar_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## April Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "wave, second wave, warns, cdc director, second, director, cdc director warns, director warns, cdc, warns second\n",
      "\n",
      "Topic  1\n",
      "us, china, help, vaccine, test, spread, lockdown, crisis, state, states\n",
      "\n",
      "Topic  2\n",
      "white, task, white house, house, task force, force, briefing, task force briefing, force briefing, house task force\n",
      "\n",
      "Topic  3\n",
      "senate, passes, billion, senate passes, relief, bill, small, passes billion, senate passes billion, package\n",
      "\n",
      "Topic  4\n",
      "contract, dem, backtracks, consultant trump, contract awarded dem, awarded dem, dem consultant trump, awarded dem consultant, contract awarded, trump asap\n",
      "\n",
      "Topic  5\n",
      "navy, carrier, aircraft carrier, aircraft, captain, alarm, raised alarm, raised, captain raised, captain raised alarm\n",
      "\n",
      "Topic  6\n",
      "trump, immigration, immigration suspension, suspension, trump immigration, trump immigration suspension, suspend immigration, suspend, trump suspend, trump suspend immigration\n",
      "\n",
      "Topic  7\n",
      "possibly worse winter, possibly worse, worse winter, cdc chief, second possibly, second possibly worse, chief, possibly, worse, chief second\n",
      "\n",
      "Topic  8\n",
      "mutated, strains, finds, study, least different, mutated least, mutated least different, least different strains, different strains, study finds\n",
      "\n",
      "Topic  9\n",
      "million, worldwide, million worldwide, top, top million, pass million, pass, surpass million, surpass, worldwide probably\n"
     ]
    }
   ],
   "source": [
    "data = apr_df.text.drop_duplicates()\n",
    "apr_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "apr_tfidf_nmf_doc_word = apr_nmf_tfidf.fit_transform(data)\n",
    "apr_nmf_model = NMF(10)\n",
    "apr_tfidf_nmf_doc_topic = apr_nmf_model.fit_transform(apr_tfidf_nmf_doc_word)\n",
    "display_topics(apr_nmf_model, apr_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "deregulations amid, deregulations, order aiming hundreds, announces executive, executive order aiming, announces executive order, aiming hundreds, order aiming, aiming, executive order\n",
      "\n",
      "Topic  1\n",
      "cost largest, bill estimated, estimated cost largest, bill estimated cost, estimated cost, cost largest stimulus, estimated, largest stimulus, largest stimulus package, package yet\n",
      "\n",
      "Topic  2\n",
      "democrats, relief, trillion, house democrats, bill, house, trillion relief, democrats trillion, house democrats trillion, democrats trillion relief\n",
      "\n",
      "Topic  3\n",
      "trump, fauci, response, dr, president, china, testing, positive, us, numbers\n",
      "\n",
      "Topic  4\n",
      "misreading data, misreading, egregious misreading data, egregious misreading, egregious, data survey, misreading data survey, cnn accused egregious, cnn accused, accused egregious misreading\n",
      "\n",
      "Topic  5\n",
      "illinois, companies involved testing, companies involved, stake companies, stake companies involved, governor family, governor family firm, illinois governor family, firm owns stake, family firm owns\n",
      "\n",
      "Topic  6\n",
      "vaccine, treatment ready, states, vaccine treatment ready, scale reopening, reopening vaccine treatment, full scale reopening, reopening vaccine, states full, scale reopening vaccine\n",
      "\n",
      "Topic  7\n",
      "us, august, model, us august, model projects, projects, projects us, projects us august, model projects us, key model\n",
      "\n",
      "Topic  8\n",
      "white house, white, house, rates spiking, white house report, house report, spiking, rates spiking heartland, spiking heartland, heartland\n",
      "\n",
      "Topic  9\n",
      "dies, spat, worker, dies spat, worker dies, worker dies spat, uk, rail worker, rail, uk rail\n"
     ]
    }
   ],
   "source": [
    "data = may_df.text.drop_duplicates()\n",
    "may_nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), binary=True)\n",
    "may_tfidf_nmf_doc_word = may_nmf_tfidf.fit_transform(data)\n",
    "may_nmf_model = NMF(10)\n",
    "may_tfidf_nmf_doc_topic = may_nmf_model.fit_transform(may_tfidf_nmf_doc_word)\n",
    "display_topics(may_nmf_model, may_nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "Not too much additional data here. It does show when certain topics are the most popular though. The next stop is to create a plot showing how topics changed over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
