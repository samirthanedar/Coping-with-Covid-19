{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/samir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/samir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy \n",
    "\n",
    "import GetOldTweets3 as got\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import gensim, spacy, logging, warnings\n",
    "import en_core_web_sm\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the tweets to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'books', 'config', 'events', 'local', 'outings']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connecting to MongoDB\n",
    "client = MongoClient()\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a new database for tweets\n",
    "db = client.admin\n",
    "\n",
    "#making a new collection in the admin database\n",
    "tweets = db.tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a list of csvs containing all the tweets I scraped using the API in command line\n",
    "csvs = ['coronavirus_tweets_dec_01_to_jan_12.csv', 'coronavirus_tweets_jan_22.csv', \n",
    "       'coronavirus_tweets_feb_02.csv', 'coronavirus_tweets_feb_12.csv', 'coronavirus_tweets_feb_21.csv',\n",
    "        'coronavirus_tweets_march_12.csv', 'coronavirus_tweets_march_22.csv', 'coronavirus_tweets_march_29.csv',\n",
    "        'coronavirus_tweets_april_02.csv' ,'coronavirus_tweets_april_21.csv', 'coronavirus_tweets_may_12.csv',\n",
    "       'coronavirus_tweets_may_19.csv']\n",
    "\n",
    "#turning each csv into a list of dictionaries and then trying to insert the tweets into MongoDB\n",
    "for url in csvs:\n",
    "    list_of_dicts = pd.read_csv('data/' + url).astype(str).to_dict(orient='records')\n",
    "    try:\n",
    "        tweets.insert_many(list_of_dicts)\n",
    "    except BulkWriteError as exc:\n",
    "        exc.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84615"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if all the tweets made it into the database\n",
    "tweets.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the data from MongoDB\n",
    "cursor = tweets.find({},{'_id':0,'permalink':0, 'geo':0,})\n",
    "tweet_list = list(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>to</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>text</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-12 23:55:37</td>\n",
       "      <td>jrbchunklight</td>\n",
       "      <td>statnews</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watch this space. Wuhan is a coronavirus like ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1216509066132049920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-12 23:45:28</td>\n",
       "      <td>coronavirus_RD</td>\n",
       "      <td>CorinaLantigua</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Todos mis tweets lo son.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1216506512958525440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-12 23:43:54</td>\n",
       "      <td>marcosarellano</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>China's mystery 'coronavirus' isn't currently ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1216506117741662211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-12 23:38:30</td>\n",
       "      <td>ImkenmacMaclean</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>China's mystery 'coronavirus' isn't currently ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1216504758569140225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-12 23:35:09</td>\n",
       "      <td>poandpo</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1 dead, 41 diagnosed with coronavirus-related ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>#Health</td>\n",
       "      <td>1216503916818522112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date         username              to replies retweets  \\\n",
       "0  2020-01-12 23:55:37    jrbchunklight        statnews       1        0   \n",
       "1  2020-01-12 23:45:28   coronavirus_RD  CorinaLantigua       1        0   \n",
       "2  2020-01-12 23:43:54   marcosarellano             nan       1        0   \n",
       "3  2020-01-12 23:38:30  ImkenmacMaclean             nan       1        0   \n",
       "4  2020-01-12 23:35:09          poandpo             nan       0        0   \n",
       "\n",
       "  favorites                                               text mentions  \\\n",
       "0         0  Watch this space. Wuhan is a coronavirus like ...      nan   \n",
       "1         2                           Todos mis tweets lo son.      nan   \n",
       "2         0  China's mystery 'coronavirus' isn't currently ...      nan   \n",
       "3         0  China's mystery 'coronavirus' isn't currently ...      nan   \n",
       "4         0  1 dead, 41 diagnosed with coronavirus-related ...      nan   \n",
       "\n",
       "  hashtags                   id  \n",
       "0      nan  1216509066132049920  \n",
       "1      nan  1216506512958525440  \n",
       "2      nan  1216506117741662211  \n",
       "3      nan  1216504758569140225  \n",
       "4  #Health  1216503916818522112  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle('uncleaned_full_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking our URLs\n",
    "urls = lambda x: re.sub(r'http\\S+', '' ,x)\n",
    "\n",
    "#taking out capitalization and digits\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "\n",
    "#removing punctuation\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "df.text = df.text.map(urls).map(alphanumeric).map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle('basic_cleaned_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('basic_cleaned_df.pkl', 'rb') as handle:\n",
    "    df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "X = df.text.drop_duplicates()\n",
    "data_list = [x for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding custom stop words for this use case\n",
    "addl_stop_words = (['coronavirus','corona virus', 'covid', 'covid-19', 'covid 19', 'corona',\n",
    "                   'virus', 'new', 'case','cases', 'deaths', 'total', 'people', 'confirmed', 'novel',\n",
    "                   'outbreak', 'pandemic', 'epidemic', 'death', 'like', 'just', 'news', 'rt', 'increasingly',\n",
    "                   'illness', 'infection', 'infected', 'diagnosed', 'reports', \"breaking\", 'reported', 'dead'\n",
    "                  ,'looks', 'know', 'big', 'type', 'make', 'unveil', 'experts', 'say', 'says', 'said', \n",
    "                    'grows', 'growing','day', 'days', \"foxnews\",'week','patient', 'hospital', 'number', \n",
    "                    'sick', 'doctor', 'next', 'health', 'first', 'even', 'press', 'youtube', 'fact', \n",
    "                    'likely', 'global', 'disease', 'thing', 'really','world', 'man', 'also', 'month', \n",
    "                    'job', 'many', 'time', 'way', 'get', 'think', 'need', 'home', 'go', 'may', 'going', 'would',\n",
    "                    'live', 'see', 'update', 'far', 'last', 'year', 'back', 'much', 'medical', 'one', 'via',\n",
    "                    'could', 'maybe', 'details', 'today', 'three', 'ninth', 'epoch', 'epoch times', 'download', 'app'\n",
    "                   ,'pron', 'daily', 'updates', 'coverage', 'fox', 'virtual', 'hall', 'programming', 'alert',\n",
    "                   'coronavirusoutbreak', 'confirm','due','die', 'gon', 'na', 'gonna', 'wan', 'wanna', 'come', 'take'\n",
    "                   , 'kill'])\n",
    "\n",
    "custom_stop_words = stopwords.words('english') + addl_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function is taken from Selva Prabhakaran's post on Machine Learning Plus which can be found here: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data = []\n",
    "for sentence in data_list:\n",
    "    data.append([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final step of getting data ready for a vectorizer\n",
    "final = []\n",
    "for sentence in data:\n",
    "    final.append(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer + LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75008, 27387)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 = CountVectorizer(stop_words=custom_stop_words,ngram_range=(1,3),min_df = 5, max_df=0.90,binary=True)\n",
    "X_cv = cv1.fit_transform(final)\n",
    "X_cv.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_df = pd.DataFrame(X_cv.toarray(), index=df.date, columns=cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041367494639639235"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_amount = 10\n",
    "lsa = TruncatedSVD(topic_amount)\n",
    "doc_topic = lsa.fit_transform(X_cv)\n",
    "sum(lsa.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function taken from Metis lecture slides to help display the topics and the top words per topic\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "china, trump, test, wuhan, spread, state, report, amp, country, president\n",
      "\n",
      "Topic  1\n",
      "trump, test, president, american, president trump, state, response, positive, test positive, house\n",
      "\n",
      "Topic  2\n",
      "test, positive, test positive, state, report, work, kit, test kit, result, cdc\n",
      "\n",
      "Topic  3\n",
      "trump, test, china, positive, test positive, wuhan, president trump, president, donald trump, donald\n",
      "\n",
      "Topic  4\n",
      "spread, stop, trump, test, stop spread, prevent, prevent spread, positive, test positive, slow\n",
      "\n",
      "Topic  5\n",
      "state, report, united, united state, spread, house, reopen, official, white, white house\n",
      "\n",
      "Topic  6\n",
      "wuhan, report, chinese, cause, pneumonia, city, wuhan china, trump, quarantine, sars\n",
      "\n",
      "Topic  7\n",
      "report, house, bill, white, white house, spread, package, stimulus, relief, democrat\n",
      "\n",
      "Topic  8\n",
      "wuhan, house, bill, white, white house, democrat, package, stimulus, relief, chinese\n",
      "\n",
      "Topic  9\n",
      "amp, american, country, house, realdonaldtrump, watch, social, infect, white, distance\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, cv1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer + NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "china, sars, flu, travel, wuhan china, outside, gt, outside china, country, sars flu\n",
      "\n",
      "Topic  1\n",
      "trump, president, president trump, response, american, donald, donald trump, administration, trump administration, briefing\n",
      "\n",
      "Topic  2\n",
      "test, positive, test positive, kit, negative, test kit, result, cdc, weinstein, harvey\n",
      "\n",
      "Topic  3\n",
      "work, help, want, well, life, use, stay, country, still, crisis\n",
      "\n",
      "Topic  4\n",
      "spread, stop, stop spread, prevent, prevent spread, country, slow, cdc, fear, slow spread\n",
      "\n",
      "Topic  5\n",
      "state, united, united state, reopen, order, governor, official, country, york, government\n",
      "\n",
      "Topic  6\n",
      "wuhan, chinese, pneumonia, cause, city, wuhan china, sars, wuhan pneumonia, authority, million\n",
      "\n",
      "Topic  7\n",
      "report, italy, hubei, province, china report, break, bring, toll, italy report, hubei province\n",
      "\n",
      "Topic  8\n",
      "house, bill, white, white house, package, stimulus, democrat, relief, senate, dems\n",
      "\n",
      "Topic  9\n",
      "amp, realdonaldtrump, response, watch, dr, lie, usa, article, vaccine, support\n"
     ]
    }
   ],
   "source": [
    "cv_nmf = CountVectorizer(stop_words = custom_stop_words,ngram_range=(1,3),min_df = 5, max_df=.9,binary=True)\n",
    "cv_nmf_doc_word = cv_nmf.fit_transform(final)\n",
    "nmf_model = NMF(10)\n",
    "cv_nmf_doc_topic = nmf_model.fit_transform(cv_nmf_doc_word)\n",
    "display_topics(nmf_model, cv_nmf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), min_df = 10, max_df=.9, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = tfidf.fit_transform(final)\n",
    "#tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=df.date, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01913866259133547"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_tfidf = TruncatedSVD(10)\n",
    "tfidf_lsa_doc_topic = lsa_tfidf.fit_transform(X_tfidf)\n",
    "sum(lsa_tfidf.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "china, trump, test, wuhan, spread, report, state, work, positive, country\n",
      "\n",
      "Topic  1\n",
      "china, wuhan, report, pneumonia, wuhan china, china report, chinese, cause, sars, outside china\n",
      "\n",
      "Topic  2\n",
      "test, positive, test positive, china, report, wuhan, pneumonia, weinstein, harvey, harvey weinstein\n",
      "\n",
      "Topic  3\n",
      "trump, china, president, president trump, response, donald, donald trump, test, administration, american\n",
      "\n",
      "Topic  4\n",
      "report, italy, state, china report, bring, italy report, county, break, report bring, toll\n",
      "\n",
      "Topic  5\n",
      "spread, state, stop, country, stop spread, united, united state, help, prevent, prevent spread\n",
      "\n",
      "Topic  6\n",
      "wuhan, chinese, pneumonia, cause, wuhan china, sars, city, wuhan pneumonia, flu, official\n",
      "\n",
      "Topic  7\n",
      "state, united, cancel, united state, quarantine, amid, china, late, fear, business\n",
      "\n",
      "Topic  8\n",
      "bad, state, flu, good, vaccine, well, country, united, united state, quarantine\n",
      "\n",
      "Topic  9\n",
      "fuck, house, white, white house, state, mask, quarantine, want, watch, face\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_tfidf, tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('lsa_tfidf_model.pkl', 'wb') as handle:\n",
    "    #pickle.dump(lsa_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "well, help, want, life, amp, right, stay, look, still, please\n",
      "\n",
      "Topic  1\n",
      "china, outside, outside china, china report, wuhan china, travel, sars, hubei, flu, pneumonia\n",
      "\n",
      "Topic  2\n",
      "test, positive, test positive, weinstein, harvey, harvey weinstein, negative, weinstein test, weinstein test positive, harvey weinstein test\n",
      "\n",
      "Topic  3\n",
      "trump, president, response, president trump, donald, donald trump, american, administration, trump administration, trump response\n",
      "\n",
      "Topic  4\n",
      "report, italy, china report, bring, italy report, report bring, break, hubei, county, bring report\n",
      "\n",
      "Topic  5\n",
      "spread, stop, stop spread, prevent, prevent spread, country, slow, slow spread, cdc, official\n",
      "\n",
      "Topic  6\n",
      "fuck, shit, bitch, hate, bro, give, everything, fuck shit, damn, miss\n",
      "\n",
      "Topic  7\n",
      "wuhan, chinese, cause, pneumonia, wuhan china, city, wuhan pneumonia, sars, mystery, pneumonia wuhan\n",
      "\n",
      "Topic  8\n",
      "state, united, united state, reopen, official, york, county, washington, cdc, governor\n",
      "\n",
      "Topic  9\n",
      "cancel, fear, mobile, congress, mobile congress, concern, congress cancel, mobile congress cancel, show, amid\n",
      "\n",
      "Topic  10\n",
      "bad, good, flu, cdc, second, wave, second wave, warns, winter, cure\n",
      "\n",
      "Topic  11\n",
      "quarantine, cruise, ship, lockdown, self, cruise ship, self quarantine, city, quarantinelife, princess\n",
      "\n",
      "Topic  12\n",
      "mask, face, wear, face mask, wear mask, wear face, protect, wear face mask, hand, use\n",
      "\n",
      "Topic  13\n",
      "house, white, white house, bill, relief, force, package, stimulus, task, democrat\n",
      "\n",
      "Topic  14\n",
      "work, employee, worker, hard, jeffreestarapproved, work together, school, company, office, employee work\n"
     ]
    }
   ],
   "source": [
    "topic_amount = 15\n",
    "nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), min_df = 5, max_df=.9, binary=True)\n",
    "tfidf_nmf_doc_word = nmf_tfidf.fit_transform(final)\n",
    "nmf_model_2 = NMF(topic_amount)\n",
    "tfidf_nmf_doc_topic = nmf_model_2.fit_transform(tfidf_nmf_doc_word)\n",
    "display_topics(nmf_model_2, nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('nmf_tfidf_model.pkl', 'wb') as handle:\n",
    "    #pickle.dump(nmf_model_2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is it! This is TF-IDF + NMF model is the best topics I have found yet. I'm going to save the resulting doc_topic matrix and use it for visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created these topic names after seeing the top tweets for each one\n",
    "Topics = ['General', 'Initial Stories', 'People testing positive', 'Trumps Response to Covid-19', 'Italy Covid-19 Outbreak'\n",
    "          ,'Stopping the spread','Anger','Coronavirus growing in China','United States Covid-19 outbreak', \n",
    "          'Covid-19 cancellations', 'Second Wave Warnings', 'Cruise and Quarantines','Wearing a mask', \n",
    "          'White House Briefings', 'Working from home']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the doc-topic matrix for later use\n",
    "save = pd.DataFrame(tfidf_nmf_doc_topic, index=X,columns=Topics)\n",
    "save['date'] = df.date\n",
    "\n",
    "#with open('nmf_tfidf_doc_topic.pkl', 'wb') as handle:\n",
    "    #pickle.dump(save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
