{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/samir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/samir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy \n",
    "\n",
    "import GetOldTweets3 as got\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from pymongo.errors import BulkWriteError\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import gensim, spacy, logging, warnings\n",
    "import en_core_web_sm\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the GetOldTweets API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch('coronavirus')\\\n",
    "                                           .setSince(\"2020-05-09\")\\\n",
    "                                           .setUntil(\"2020-05-10\").setMaxTweets(1000).setNear('New York').setWithin('50mi')\n",
    "tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list = []\n",
    "for idx,x in enumerate(tweet):\n",
    "    if x.retweets>1:\n",
    "        tweet_list.append(x.text)\n",
    "        \n",
    "len(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list_str = [str(item) for item in tweet_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Jill & Erykah stay on long enough, they might ...\n",
       "1      President Trump announces the federal governme...\n",
       "2      They're calling it \"Covid toe\": painful red or...\n",
       "3      Cuomo: New coronavirus testing sites to open i...\n",
       "4      Trump, Who Called The Pandemic A Hoax, Is Now ...\n",
       "                             ...                        \n",
       "230    Health care workers are risking their lives ‚Äî ...\n",
       "231    Paxos cofounder explains why Wall Street's plu...\n",
       "232    NEW: President Trump announces the federal gov...\n",
       "233    NEW: President Trump announces the federal gov...\n",
       "234    Running on empty: Coronavirus has changed the ...\n",
       "Name: stuff, Length: 235, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tweet_list_str)\n",
    "df.columns = ['stuff']\n",
    "df.stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>01221</th>\n",
       "      <th>020</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>0p0vm5o</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>yorkers</th>\n",
       "      <th>young</th>\n",
       "      <th>youthwithyouep16</th>\n",
       "      <th>youtu</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zulu</th>\n",
       "      <th>ùôäùô£ùôö</th>\n",
       "      <th>ùô©ùôùùôûùôßùôô</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 1916 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  01221  020  04  05  06  08  09  0p0vm5o  10  ...  yorkers  young  \\\n",
       "0    0      0    0   0   0   0   0   0        0   0  ...        0      0   \n",
       "1    0      0    0   0   0   0   0   0        0   0  ...        0      0   \n",
       "2    0      0    0   0   0   0   0   0        0   0  ...        0      1   \n",
       "3    0      0    0   0   0   0   0   0        0   0  ...        0      0   \n",
       "4    0      0    0   0   0   0   0   0        0   0  ...        0      0   \n",
       "\n",
       "   youthwithyouep16  youtu  youtube  zero  zone  zulu  ùôäùô£ùôö  ùô©ùôùùôûùôßùôô  \n",
       "0                 0      0        0     0     0     0    0      0  \n",
       "1                 0      0        0     0     0     0    0      0  \n",
       "2                 0      0        0     0     0     0    0      0  \n",
       "3                 0      0        0     0     0     0    0      0  \n",
       "4                 0      0        0     0     0     0    0      0  \n",
       "\n",
       "[5 rows x 1916 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv1 = cv1.fit_transform(df.stuff)\n",
    "pd.DataFrame(df_cv1.toarray(), columns=cv1.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the tweets to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'books', 'config', 'events', 'local', 'outings']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connecting to MongoDB\n",
    "client = MongoClient()\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a new database for tweets\n",
    "db = client.admin\n",
    "\n",
    "#making a new collection in the admin database\n",
    "tweets = db.tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a list of csvs containing all the tweets I scraped using the API in command line\n",
    "csvs = ['coronavirus_tweets_dec_01_to_jan_12.csv', 'coronavirus_tweets_jan_22.csv', \n",
    "       'coronavirus_tweets_feb_02.csv', 'coronavirus_tweets_feb_12.csv', 'coronavirus_tweets_feb_21.csv',\n",
    "        'coronavirus_tweets_march_12.csv', 'coronavirus_tweets_march_22.csv', 'coronavirus_tweets_march_29.csv',\n",
    "        'coronavirus_tweets_april_02.csv' ,'coronavirus_tweets_april_21.csv', 'coronavirus_tweets_may_12.csv',\n",
    "       'coronavirus_tweets_may_19.csv']\n",
    "\n",
    "#turning each csv into a list of dictionaries and then trying to insert the tweets into MongoDB\n",
    "for url in csvs:\n",
    "    list_of_dicts = pd.read_csv(url).astype(str).to_dict(orient='records')\n",
    "    try:\n",
    "        tweets.insert_many(list_of_dicts)\n",
    "    except BulkWriteError as exc:\n",
    "        exc.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84615"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if all the tweets made it into the database\n",
    "tweets.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps:**\n",
    "1. Tokenize data\n",
    "1. Clean data\n",
    "1. Lemmatize\n",
    "1. Countvectorizer or TF-IDF\n",
    "1. Topic Modeling with LSA or NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84615"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the data from MongoDB\n",
    "#cursor = tweets.find()\n",
    "cursor = tweets.find({},{'_id':0,'permalink':0, 'geo':0,})\n",
    "tweet_list = list(cursor)\n",
    "len(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>to</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>text</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-12 23:55:37</td>\n",
       "      <td>jrbchunklight</td>\n",
       "      <td>statnews</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watch this space. Wuhan is a coronavirus like ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1216509066132049920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-12 23:45:28</td>\n",
       "      <td>coronavirus_RD</td>\n",
       "      <td>CorinaLantigua</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Todos mis tweets lo son.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1216506512958525440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-12 23:43:54</td>\n",
       "      <td>marcosarellano</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>China's mystery 'coronavirus' isn't currently ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1216506117741662211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-12 23:38:30</td>\n",
       "      <td>ImkenmacMaclean</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>China's mystery 'coronavirus' isn't currently ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1216504758569140225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-12 23:35:09</td>\n",
       "      <td>poandpo</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1 dead, 41 diagnosed with coronavirus-related ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>#Health</td>\n",
       "      <td>1216503916818522112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date         username              to replies retweets  \\\n",
       "0  2020-01-12 23:55:37    jrbchunklight        statnews       1        0   \n",
       "1  2020-01-12 23:45:28   coronavirus_RD  CorinaLantigua       1        0   \n",
       "2  2020-01-12 23:43:54   marcosarellano             nan       1        0   \n",
       "3  2020-01-12 23:38:30  ImkenmacMaclean             nan       1        0   \n",
       "4  2020-01-12 23:35:09          poandpo             nan       0        0   \n",
       "\n",
       "  favorites                                               text mentions  \\\n",
       "0         0  Watch this space. Wuhan is a coronavirus like ...      nan   \n",
       "1         2                           Todos mis tweets lo son.      nan   \n",
       "2         0  China's mystery 'coronavirus' isn't currently ...      nan   \n",
       "3         0  China's mystery 'coronavirus' isn't currently ...      nan   \n",
       "4         0  1 dead, 41 diagnosed with coronavirus-related ...      nan   \n",
       "\n",
       "  hashtags                   id  \n",
       "0      nan  1216509066132049920  \n",
       "1      nan  1216506512958525440  \n",
       "2      nan  1216506117741662211  \n",
       "3      nan  1216504758569140225  \n",
       "4  #Health  1216503916818522112  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('uncleaned_full_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking our URLs\n",
    "urls = lambda x: re.sub(r'http\\S+', '' ,x)\n",
    "\n",
    "#taking out capitalization and digits\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "\n",
    "#removing punctuation\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "df.text = df.text.map(urls).map(alphanumeric).map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('basic_cleaned_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('basic_cleaned_df.pkl', 'rb') as handle:\n",
    "    df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "X = df.text.drop_duplicates()\n",
    "data_list = [x for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = df.mask(df.text.duplicated(keep='first'),)\n",
    "#new_df = new_df.dropna()\n",
    "#new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding custom stop words for this use case\n",
    "addl_stop_words = (['coronavirus','corona virus', 'covid', 'covid-19', 'covid 19', 'corona',\n",
    "                   'virus', 'new', 'case','cases', 'deaths', 'total', 'people', 'confirmed', 'novel',\n",
    "                   'outbreak', 'pandemic', 'epidemic', 'death', 'like', 'just', 'news', 'rt', 'increasingly',\n",
    "                   'illness', 'infection', 'infected', 'diagnosed', 'reports', \"breaking\", 'reported', 'dead'\n",
    "                  ,'looks', 'know', 'big', 'type', 'make', 'unveil', 'experts', 'say', 'says', 'said', \n",
    "                    'grows', 'growing','day', 'days', \"foxnews\",'week','patient', 'hospital', 'number', \n",
    "                    'sick', 'doctor', 'next', 'health', 'first', 'even', 'press', 'youtube', 'fact', \n",
    "                    'likely', 'global', 'disease', 'thing', 'really','world', 'man', 'also', 'month', \n",
    "                    'job', 'many', 'time', 'way', 'get', 'think', 'need', 'home', 'go', 'may', 'going', 'would',\n",
    "                    'live', 'see', 'update', 'far', 'last', 'year', 'back', 'much', 'medical', 'one', 'via',\n",
    "                    'could', 'maybe', 'details', 'today', 'three', 'ninth', 'epoch', 'epoch times', 'download', 'app'\n",
    "                   ,'pron', 'daily', 'updates', 'coverage', 'fox', 'virtual', 'hall', 'programming', 'alert',\n",
    "                   'coronavirusoutbreak', 'confirm','due','die', 'gon', 'na', 'gonna', 'wan', 'wanna', 'come', 'take'\n",
    "                   , 'kill'])\n",
    "\n",
    "custom_stop_words = stopwords.words('english') + addl_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function is taken from Selva Prabhakaran's post on Machine Learning Plus which can be found here: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data = []\n",
    "for sentence in data_list:\n",
    "    data.append([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final step of getting data ready for a vectorizer\n",
    "final = []\n",
    "for sentence in data:\n",
    "    final.append(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer + LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75008, 27873)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1 = CountVectorizer(stop_words=custom_stop_words,ngram_range=(1,3),min_df = 5, max_df=0.90,binary=True)\n",
    "X_cv = cv1.fit_transform(final)\n",
    "X_cv.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_df = pd.DataFrame(X_cv.toarray(), index=df.date, columns=cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041132508265173014"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_amount = 10\n",
    "lsa = TruncatedSVD(topic_amount)\n",
    "doc_topic = lsa.fit_transform(X_cv)\n",
    "sum(lsa.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function taken from lecture slides to help display the topics and the top words per topic\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "china, trump, test, wuhan, spread, state, take, amp, report, country\n",
      "\n",
      "Topic  1\n",
      "trump, test, president, american, president trump, response, state, take, positive, house\n",
      "\n",
      "Topic  2\n",
      "test, positive, test positive, state, report, work, kit, test kit, result, cdc\n",
      "\n",
      "Topic  3\n",
      "trump, test, china, positive, test positive, president, president trump, wuhan, donald trump, donald\n",
      "\n",
      "Topic  4\n",
      "spread, stop, trump, stop spread, test, prevent, state, prevent spread, slow, cdc\n",
      "\n",
      "Topic  5\n",
      "state, report, united, united state, house, reopen, bill, white, white house, official\n",
      "\n",
      "Topic  6\n",
      "wuhan, report, chinese, cause, pneumonia, die, city, wuhan china, sars, infect\n",
      "\n",
      "Topic  7\n",
      "take, state, united, united state, seriously, care, chinese, take care, action, take seriously\n",
      "\n",
      "Topic  8\n",
      "report, house, take, bill, white, white house, package, stimulus, spread, democrat\n",
      "\n",
      "Topic  9\n",
      "wuhan, bill, house, help, business, work, relief, pneumonia, package, cause\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, cv1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer + NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "trump, president, president trump, response, american, donald, donald trump, administration, trump administration, briefing\n",
      "\n",
      "Topic  1\n",
      "china, sars, flu, travel, wuhan china, outside, gt, outside china, sars flu, country\n",
      "\n",
      "Topic  2\n",
      "test, positive, test positive, kit, negative, test kit, result, weinstein, cdc, harvey\n",
      "\n",
      "Topic  3\n",
      "amp, work, help, die, come, want, well, life, use, stay\n",
      "\n",
      "Topic  4\n",
      "spread, stop, stop spread, prevent, country, prevent spread, slow, fear, cdc, slow spread\n",
      "\n",
      "Topic  5\n",
      "state, united, united state, reopen, order, governor, official, country, government, york\n",
      "\n",
      "Topic  6\n",
      "wuhan, chinese, pneumonia, cause, city, wuhan china, sars, wuhan pneumonia, authority, million\n",
      "\n",
      "Topic  7\n",
      "take, care, seriously, action, take care, take seriously, look, measure, away, drug\n",
      "\n",
      "Topic  8\n",
      "report, italy, hubei, province, china report, break, bring, toll, italy report, hubei province\n",
      "\n",
      "Topic  9\n",
      "house, bill, white, white house, package, stimulus, democrat, relief, senate, dems\n"
     ]
    }
   ],
   "source": [
    "cv_nmf = CountVectorizer(stop_words = custom_stop_words,ngram_range=(1,3),min_df = 5, max_df=.9,binary=True)\n",
    "cv_nmf_doc_word = cv_nmf.fit_transform(final)\n",
    "nmf_model = NMF(10)\n",
    "cv_nmf_doc_topic = nmf_model.fit_transform(cv_nmf_doc_word)\n",
    "display_topics(nmf_model, cv_nmf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), min_df = 10, max_df=.9, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = tfidf.fit_transform(final)\n",
    "#tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=df.date, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019439552496429528"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_tfidf = TruncatedSVD(10)\n",
    "tfidf_lsa_doc_topic = lsa_tfidf.fit_transform(X_tfidf)\n",
    "sum(lsa_tfidf.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "china, trump, test, wuhan, spread, report, take, state, die, come\n",
      "\n",
      "Topic  1\n",
      "china, wuhan, report, pneumonia, wuhan china, china report, cause, sars, chinese, outside china\n",
      "\n",
      "Topic  2\n",
      "test, positive, test positive, china, report, wuhan, pneumonia, weinstein, harvey, harvey weinstein\n",
      "\n",
      "Topic  3\n",
      "trump, china, president, president trump, response, donald, donald trump, american, administration, test\n",
      "\n",
      "Topic  4\n",
      "report, state, italy, china report, bring, italy report, county, report bring, united, united state\n",
      "\n",
      "Topic  5\n",
      "na, gon, gon na, report, die, fuck, come, wan na, wan, work\n",
      "\n",
      "Topic  6\n",
      "spread, na, gon, gon na, china, die, stop, trump, bad, stop spread\n",
      "\n",
      "Topic  7\n",
      "die, wuhan, chinese, spread, cause, lockdown, pneumonia, infect, american, flu\n",
      "\n",
      "Topic  8\n",
      "wuhan, spread, na, gon, gon na, chinese, trump, come, well, quarantine\n",
      "\n",
      "Topic  9\n",
      "state, house, united, united state, take, na, gon, gon na, white, white house\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_tfidf, tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lsa_tfidf_model.pkl', 'wb') as handle:\n",
    "    pickle.dump(lsa_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samir/opt/anaconda3/envs/metis/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['19', 'times'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "well, help, want, life, amp, right, stay, look, still, please\n",
      "\n",
      "Topic  1\n",
      "china, outside, outside china, china report, wuhan china, travel, sars, hubei, flu, pneumonia\n",
      "\n",
      "Topic  2\n",
      "test, positive, test positive, weinstein, harvey, harvey weinstein, negative, weinstein test, weinstein test positive, harvey weinstein test\n",
      "\n",
      "Topic  3\n",
      "trump, president, response, president trump, donald, donald trump, american, administration, trump administration, trump response\n",
      "\n",
      "Topic  4\n",
      "report, italy, china report, bring, italy report, report bring, break, hubei, county, bring report\n",
      "\n",
      "Topic  5\n",
      "spread, stop, stop spread, prevent, prevent spread, country, slow, slow spread, cdc, official\n",
      "\n",
      "Topic  6\n",
      "state, united, united state, reopen, official, york, county, washington, cdc, governor\n",
      "\n",
      "Topic  7\n",
      "wuhan, chinese, cause, pneumonia, wuhan china, city, wuhan pneumonia, sars, mystery, pneumonia wuhan\n",
      "\n",
      "Topic  8\n",
      "fuck, shit, bitch, hate, bro, give, everything, fuck shit, damn, miss\n",
      "\n",
      "Topic  9\n",
      "cancel, fear, mobile, congress, mobile congress, concern, congress cancel, mobile congress cancel, show, amid\n",
      "\n",
      "Topic  10\n",
      "work, employee, worker, hard, jeffreestarapproved, school, work together, company, office, employee work\n",
      "\n",
      "Topic  11\n",
      "quarantine, cruise, ship, lockdown, self, cruise ship, self quarantine, city, quarantinelife, princess\n",
      "\n",
      "Topic  12\n",
      "mask, face, wear, face mask, wear mask, wear face, protect, wear face mask, hand, use\n",
      "\n",
      "Topic  13\n",
      "house, white, white house, bill, relief, force, package, stimulus, task, democrat\n",
      "\n",
      "Topic  14\n",
      "bad, good, flu, cdc, second, wave, second wave, warns, winter, cure\n"
     ]
    }
   ],
   "source": [
    "topic_amount = 15\n",
    "nmf_tfidf = TfidfVectorizer(stop_words=custom_stop_words,ngram_range=(1,3), min_df = 5, max_df=.9, binary=True)\n",
    "tfidf_nmf_doc_word = nmf_tfidf.fit_transform(final)\n",
    "nmf_model_2 = NMF(topic_amount)\n",
    "tfidf_nmf_doc_topic = nmf_model_2.fit_transform(tfidf_nmf_doc_word)\n",
    "display_topics(nmf_model_2, nmf_tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nmf_tfidf_model.pkl', 'wb') as handle:\n",
    "    pickle.dump(nmf_model_2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is it! This is TF-IDF + NMF model is the best topics I have found yet. I'm going to save the resulting doc_topic matrix and use it for visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created these topic names after seeing the top tweets for each one\n",
    "Topics = ['General', 'Initial Stories', 'People testing positive', 'Trumps Response to Covid-19', 'Italy Covid-19 Outbreak'\n",
    "          ,'Stopping the spread','United States Covid-19 outbreak','Coronavirus growing in China','Anger', \n",
    "          'Covid-19 cancellations', 'Working from home', 'Cruise and Quarantines', 'Wearing a mask', \n",
    "          'White House Briefings', 'Second Wave Warnings']\n",
    "\n",
    "column_titles = []\n",
    "for i in range(1,topic_amount+1):\n",
    "    column_titles.append('component_' + str(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the doc-topic matrix for later use\n",
    "save = pd.DataFrame(tfidf_nmf_doc_topic, index=X,columns=Topics)\n",
    "save['date'] = new_df.date\n",
    "\n",
    "with open('nmf_tfidf_doc_topic.pkl', 'wb') as handle:\n",
    "    pickle.dump(save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts so far:**\n",
    "* TFIDF seems better than CV\n",
    "* I have topics now but the TFIDF topics are basically just the biggest news headlines so far. This isn't super helpful because we probably could have guessed these just by briefly following the news\n",
    "* I feel like the custom processed NMF model is actually the best for public perception and the TFIDF topics are just the top news stories\n",
    "* I have to find a business use case or a question to answer soon because otherwise this analysis is directionless\n",
    "* As of now, I'm thinking I should focus on the following: how has public perception of the government shutdown changed over time?\n",
    "* I could also take out the news stories by taking out retweets or even by taking out tweets with many retweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps:**\n",
    "1. Try Corex to form cluster around government shutdown?\n",
    "1. Scattertext to generate some hypotheses that I can then test in data?\n",
    "1. Show how sentiment on topics changed over time\n",
    "1. distribution of topics over time?\n",
    "1. What are people complaining about more?\n",
    "1. use retweets to subset data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
